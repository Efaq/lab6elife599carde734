---
title: "lab_report_knapsack"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lab6elife599carde734}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
In this vignette, we are going to work with different knapsack problem solvers. First, we import the relevant libraries and create our dataset.
```{r setup}
library(lab6elife599carde734)

RNGversion(min(as.character(getRversion()),"3.5.3"))
set.seed(42, kind = "Mersenne-Twister", normal.kind = "Inversion")
n <- 2000
knapsack_objects <-
data.frame(
w=sample(1:4000, size = n, replace = TRUE),
v=runif(n = n, 0, 10000)
)
```
## Brute Force

Some examples of the brute force algorithm follow below:

```{r}
brute_force_knapsack(x = knapsack_objects[1:8,], W = 3500)
brute_force_knapsack(x = knapsack_objects[1:12,], W = 3500)
brute_force_knapsack(x = knapsack_objects[1:8,], W = 2000)
brute_force_knapsack(x = knapsack_objects[1:12,], W = 2000)
```
Question: How much time does it takes to run the algorithm for n = 16 objects?
```{r}
sample_size = 1 #This value is 1 to keep the vignette quick to run, but should be larger to reduce the variance of the resulting times
t1 = system.time({for (i in 1:sample_size) brute_force_knapsack(x = knapsack_objects[1:16,], W = 3500)})/sample_size
print("User time:")
t1[[1]]
print("System time:")
t1[[2]]
```
Question: What performance gain could you get by parallelizing brute force search?
```{r}
sample_size = 1 #This value is 1 to keep the vignette quick to run, but should be larger to reduce the variance of the resulting times
t2 = system.time({for (i in 1:sample_size) brute_force_knapsack(x = knapsack_objects[1:16,], W = 3500, parallel=TRUE)})/sample_size
print("User time:")
t2[[1]]
print("System time:")
t2[[2]]
```
So we can notice that the user time gets shorter, at the cost of some increase in system time. The overall benefit is though positive.

It can be noted (see below in Performance assessment) that the relative improvement is much bigger when the total runtime is longer, because overheads with core allocation and other boilerplate activities become less important in comparison with the total running time.

## Dynamic programming

Some examples of the dynamic programming approach follow below:

```{r}
knapsack_dynamic(x = knapsack_objects[1:8,], W = 3500)
knapsack_dynamic(x = knapsack_objects[1:12,], W = 3500)
knapsack_dynamic(x = knapsack_objects[1:8,], W = 2000)
knapsack_dynamic(x = knapsack_objects[1:12,], W = 2000)
```

Question: How much time does it takes to run the algorithm for n = 500 objects?

```{r}
sample_size = 1 #This value is 1 to keep the vignette quick to run, but should be larger to reduce the variance of the resulting times
t3 = system.time({for (i in 1:sample_size) knapsack_dynamic(x = knapsack_objects[1:500,], W = 3500)})/sample_size
print("User time:")
t3[[1]]
print("System time:")
t3[[2]]

```
We can notice that, even with an increase in objects from 16 to 500, the time used by the dynamic programming strategy for 500 objects is in the same order of magnitude as the brute force approach for 16 objects, confirming that the dynamic programming solution scales much better.

## Greedy heuristic

Some examples of the greedy heuristic approach follow below:

```{r}
greedy_knapsack(x = knapsack_objects[1:800,], W = 3500)
greedy_knapsack(x = knapsack_objects[1:1200,], W = 2000)
```
Question: How much time does it take to run the algorithm for n = 1000000 objects?


In order to run the algorithm for a larger number of objects, we're going to resample the dataset:

```{r}
n <- 1000000
knapsack_objects_extended <-
data.frame(
w=sample(1:4000, size = n, replace = TRUE),
v=runif(n = n, 0, 10000)
)
```
Now we can observe how fast the algorithm is for 1000000 objects:

```{r}
sample_size = 1 #This value is 1 to keep the vignette quick to run, but should be larger to reduce the variance of the resulting times
t4 = system.time({for (i in 1:sample_size) greedy_knapsack(x = knapsack_objects_extended, W = 3500)})/sample_size
print("User time:")
t4[[1]]
print("System time:")
t4[[2]]
```

We can notice that the algorithm is extremely fast, at the cost of not guaranteeing an optimal solution.


## Performance assessment and code optimization

Question: What performance gain could you get by trying to improve your code?

The main target of our performance enhancement adventure was to understand why the brute force algorithm was running so slowly. In a first correct implementation, although passing tests, the algorithm was taking in the order of 2500 seconds to deal with 16 elements, vs around 420 seconds when parallelized. Clearly there was something in the core of the algorithm that needed to be optimized.

When profiling the code, we found the clear bottleneck:

```{r eval = FALSE}
brute_force_df[["value"]] = lapply_func(
  input = row.names(brute_force_df),
  func = function(inp) {
    brute_force_df[[inp, "total_value"]] * brute_force_df[[inp, "is_feasible"]]
  }
)
```
This code means to calculate the effective value of each solution by multiplying the unconstrained solution value by 1 if it's feasible or by 0 if it isn't. As an example, for 14 objects, while the rest of the code was executing in less than a second, this part alone was taking around 130 seconds.

After identifying the issue, we solved it by rewriting the code to:
```{r eval = FALSE}
brute_force_df[["value"]] = unlist(brute_force_df[["total_value"]]) * unlist(brute_force_df[["is_feasible"]])
```
With this change, this part of the algorithm works almost instantly.

The legacy code, which, be advised, runs very slowly, is still available through the function brute_force_knapsack_slow.


